{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nearest Neighbor Classification\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">Nearest neighbor (NN) classification is a non-parametric and\n",
    "a slow learning algorithm. Since it uses no assumption for the\n",
    "underlying data, it is a non-parametric method.</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">The structure of the model is determined directly from the\n",
    "dataset. This is helpful in those real-world datasets which\n",
    "do not follow simple assumptions. It learns slowly because it\n",
    "does not need any training data points for the creation of the\n",
    "model. It uses all of the training data in the testing phase. This\n",
    "makes the testing phase slower and memory consuming.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\">The most used algorithm of this family of classifiers is called\n",
    "k-nearest neighbor (KNN), where K represents the number of\n",
    "nearest neighbors to consider for classification. K is usually\n",
    "taken as a small odd number.</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">For instance, in a two-class problem given in figure below, we\n",
    "want to predict the label for the test point with the question\n",
    "marks, ?. If we take K = 1, we have to find one training example\n",
    "closest to this test point. To classify, we assign the label of this\n",
    "nearest point to the test point.</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"Images/knn.png\" style=\"margin:auto\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\">For any other arbitrary value of K, we find the K closest point\n",
    "to the test point, and then classify the test point by a majority\n",
    "vote of its K neighbors. Each training point in the K closest points votes for its class. The class with the most votes in the\n",
    "neighborhood is taken as the predicted class for the test point.\n",
    "To find points closest or similar to the test point, we find the\n",
    "distance between points. The steps to classify a test point by\n",
    "KNN are as follows:</div>\n",
    "\n",
    "* Calculate distance\n",
    "* Find closest neighbors\n",
    "* Vote for labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">To implement a KNN classifier in Python, we first import the\n",
    "libraries and packages.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#We load the Digits dataset and split it into test and training\n",
    "#sets using the following script.\n",
    "\n",
    "# Train the model using the training sets\n",
    "\n",
    "\n",
    "\n",
    "#Note that we split our dataset into a ratio of 75:25 for training:test sets.\n",
    "\n",
    "#use the KNN model for training by specifying the input and output variables of the training set as follows.\n",
    "\n",
    "\n",
    "\n",
    "#Predict Output\n",
    "\n",
    "\n",
    "#The following section of the code displays the results using\n",
    "#the confusion matrix.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\">The predicted and actual labels are shown on the x and y-axis\n",
    "of the confusion matrix, respectively. The diagonal entries on\n",
    "the confusion matrix represent correct classification results.\n",
    "It can be observed that most digits are correctly classified\n",
    "by the model. However, occasional misclassified results are\n",
    "shown on the off-diagonal entries of the matrix. The output of\n",
    "the model shows an accuracy of 98.67 percent.</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\"><b>Advantages and Applicability:</b> Nearest neighbor classification\n",
    "is very easy to implement by just specifying the value of\n",
    "neighbors and a suitable distance function, e.g., Euclidean\n",
    "distance.</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">The nearest neighbor classifier does not learn anything in the\n",
    "training period. This is referred to as instance-based learning.\n",
    "Classification using the nearest neighbor is accomplished\n",
    "by storing the whole training dataset. This makes nearest\n",
    "neighbor classifiers much faster than logistic regression and\n",
    "other classification models. Since no training is required\n",
    "before making predictions, new data can be added easily to\n",
    "the algorithm without affecting its accuracy.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\"><b>Limitations:</b> It is very slow in the testing phase; thus, it is not\n",
    "suitable for large datasets because calculating the distance\n",
    "between the test point and each training point takes a large\n",
    "amount of time for large datasets.</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">Furthermore, algorithms based on the nearest neighbor are\n",
    "sensitive to noise, outliers, and missing feature values. We\n",
    "have to remove outliers and impute missing values before\n",
    "applying this class of algorithms to our dataset.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naïve Bayes’ Classification\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">Naive Bayes is one of the most fundamental classification\n",
    "algorithms. It is based on Bayes’ Theorem of probability.\n",
    "A basic assumption used by a Naive Bayes’ classifier is the\n",
    "independence of features. This assumption is considered\n",
    "naïve, which simplifies computations. In terms of probability,\n",
    "this assumption is called class conditional independence.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\">To understand Bayes’ theorem, we describe the conditional\n",
    "probability that is defined as the likelihood of occurrence of an\n",
    "event based on the occurrence of a previous event. Conditional\n",
    "probability is calculated by multiplying the probability of the\n",
    "preceding event by the updated probability of the conditional\n",
    "event. For example, let us define events A and B:</div>\n",
    "\n",
    "* Event A: It is raining outside, and let it has a 0.4 (40 percent) chance of raining today. The probability of event A is P(A) = 0.4.\n",
    "* Event B: A person needs to go outside, and let it has a probability P(B) = 0.3 (30 percent).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\"><b>Joint probability:</b> Let the probability that both events happen\n",
    "simultaneously is 0.2 or 20 percent. It is written as P(A and B)\n",
    "or P(A⋂B) and is known as the joint probability of A and B.\n",
    "The symbol ⋂ is for the intersection of the events.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\"><b>Conditional probability:</b> Now, we are interested to know the\n",
    "probability or chances of occurrence of rain given the person\n",
    "has come out. The probability of rain given the person went\n",
    "out is the conditional probability P(A|B) that can be given as,</div>\n",
    "\n",
    "$$ P(A|B) = P(A\\cap B)/P(B) = 0.2/0.3 = 0.66 = 0.66% $$\n",
    "\n",
    "<div style=\"text-align: justify\">Besides P(A|B), there is another conditional probability related\n",
    "to the event: the probability of occurrence of event B given A has already occurred, P(B|A). Bayes’ theorem converts one\n",
    "conditional probability to the other conditional probability. It\n",
    "is given as</div>\n",
    "\n",
    "$$P(B|A)= (P(A|B) P(B))/(P(A)) = (0.66)(0.3)/0.4 = 49.5% $$\n",
    "\n",
    "<div style=\"text-align: justify\">It is evident from this example that generally, P(A|B) is not\n",
    "equal to P(B|A).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\">To implement the Naïve Bayes’ algorithm in Python, we may\n",
    "write the following script.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "\n",
    "\n",
    "#Note:\n",
    "#Suppose we want to predict whether we should play based on\n",
    "#weather conditions and temperature readings. Weather and\n",
    "#temperature become our features, and the decision to play\n",
    "#becomes the target variable or the output. We assign features\n",
    "#and label variables as follows.\n",
    "\n",
    "# Assigning features and label variables\n",
    "weather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',\n",
    "'Rainy','Sunny','Overcast','Overcast','Rainy']\n",
    "temp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']\n",
    "\n",
    "play=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']\n",
    "\n",
    "\n",
    "#Note:\n",
    "#Since weather and temperature are given as strings, it is difficult\n",
    "#to train our model on strings. We transform our features and\n",
    "#the target variable to numeric data as follows.\n",
    "\n",
    "#creating label Encoder\n",
    "\n",
    "\n",
    "# Converting string labels into numbers.\n",
    "\n",
    "\n",
    "# Encode temp and play columns. Converting string labels into numbers\n",
    "\n",
    "\n",
    "#combining features weather and temp in a single variable (list of tuples).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate a model using naive bayes classifier in the following steps:\n",
    "# 1. Create naive bayes classifier\n",
    "# 2. Fit the dataset on classifier\n",
    "# 3. Perform prediction\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "\n",
    "\n",
    "# Train the model using the training sets\n",
    "\n",
    "\n",
    "#Predict Output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\">We convert string feature values and output labels into integers\n",
    "    using label encoding method <b>preprocessing.LabelEncoder().</b>\n",
    "    The function <b>fit_transform (weather)</b> converts string labels\n",
    "for weather conditions into numbers.</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">We generate a model using Naive Bayes’ classifier by the\n",
    "following steps:</div>\n",
    "\n",
    "1. Create naive Bayes’ classifier\n",
    "2. Fit the dataset on classifier\n",
    "3. Perform prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(features,label)\n",
    "\n",
    "#Predict Output\n",
    "predicted= model.predict([[0,2]]) # 0:Overcast, 2:Mild\n",
    "print ('Predicted Value:', predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(features,label)\n",
    "\n",
    "#Predict Output\n",
    "predicted= model.predict([[0,2]]) # 0:Overcast, 2:Mild\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\">To predict a test point, we have used the function predict ().</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\"><b>Advantages and Applicability:</b> Naïve Bayes’ is easy to\n",
    "implement and interpret. It performs better compared to\n",
    "other similar models when the input features are independent\n",
    "of each other. A small amount of training data is sufficient for\n",
    "this model to estimate the test data.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\"><b>Limitations:</b> The main limitation of Naïve Baye’ is the\n",
    "assumption of independence between the independent\n",
    "variables. If features are dependent, this algorithm cannot\n",
    "be applied. Dimensionality reduction techniques can be used\n",
    "to transform the features into a set of independent features\n",
    "before applying the Naïve Bayes’ classifier.</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
