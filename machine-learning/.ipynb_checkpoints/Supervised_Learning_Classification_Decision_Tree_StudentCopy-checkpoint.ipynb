{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">A decision tree is similar to a flowchart in which the nodes\n",
    "represent features, the branches represent a decision rule, and\n",
    "each leaf node represents the result or outcome of applying\n",
    "the rules on these features. In a decision tree, the topmost\n",
    "node is known as the root or parent node.</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">The learning in decision trees is accomplished by partitioning\n",
    "at each internal node on the basis of the values of features. The\n",
    "visualization of a tree as a flowchart mimics human thinking.\n",
    "Thus, decision trees are easy to understand and interpret. The\n",
    "paths from the root to the leaf represent classification rules.\n",
    "Decision trees are mostly used for classification; however,\n",
    "these can also be utilized for regression.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"Images/decisiontree.png\" style=\"margin:auto\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\">Decision trees learn from data with a set of if-then-else rules.\n",
    "The decision tree is a non-parametric method that does not\n",
    "depend upon the assumptions of probability distributions.\n",
    "The basic point behind any decision tree algorithm is along\n",
    "these lines:</div>\n",
    "\n",
    "- Select the best feature at each node using some kind of feature selection measure to split the observations given in the data. The best feature is the one that best separates the given data.\n",
    "- This feature becomes a decision node, and we break the dataset into smaller subsets at this node.\n",
    "- Start building a tree by repeating this process recursively for each child node until either there are no more remaining features or there are no more observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\">Let us assume we want to play tennis on a particular day. How\n",
    "do we decide whether to play or not? We check the weather\n",
    "if it is hot or cold, check the speed of the wind and humidity.\n",
    "We take all these factors into account to decide if we shall\n",
    "play or not.</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">We gather data for temperature, wind, and humidity for a few\n",
    "days and make a decision tree similar to the one shown in\n",
    "figure below. All possible paths from the root to the leaf nodes\n",
    "represent a set of rules that lead to the final decision of playing\n",
    "or not playing tennis.</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"Images/DTexample.png\" style=\"margin:auto\"/> \n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">As an example, consider the leftmost path from the root\n",
    "Weather all the way down to No. The decision rule in this path\n",
    "says that if Weather is Sunny and the Humidity is High, we\n",
    "shall not play.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\">To implement the decision tree algorithm for classification in\n",
    "Python, we may write the following script.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes to predict:  ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# code for decision tree based classification \n",
    "from sklearn import datasets\n",
    "#Loading the iris data\n",
    "# Loading dataset\n",
    "iris_data = datasets.load_iris()\n",
    "\n",
    "print(\"Classes to predict: \", iris_data.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in the data:  150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Extracting data attributes\n",
    "X = iris_data.data\n",
    "### Extracting target/ class labels\n",
    "y = iris_data.target\n",
    "\n",
    "print(\"Number of examples in the data: \", X.shape[0])\n",
    "\n",
    "\n",
    "#Using the train_test_split to create train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1, test_size = 0.25)\n",
    "\n",
    "\n",
    "#Importing the Decision tree classifier from the sklearn library.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(criterion = 'entropy')\n",
    "\n",
    "#Training the decision tree classifier. \n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score on train data:  1.0\n",
      "Accuracy Score on test data:  0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "#Predicting labels on the test set.\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#Importing the accuracy metric from sklearn.metrics library\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy Score on train data: \", accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)))\n",
    "print(\"Accuracy Score on test data: \", accuracy_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\">The output indicates that the model performs 100 percent on\n",
    "the training data. However, its generalization power to predict\n",
    "test points decreases to about 95 percent. This accuracy is\n",
    "usually considered acceptable because the model generalizes\n",
    "well to unseen examples.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\"><b>Ensemble methods:</b> The predicting power of decision trees can\n",
    "be enhanced by growing multiple trees on a slightly different\n",
    "version of the training data. The resulting class of methods is\n",
    "known as ensemble methods. To predict the output or class\n",
    "for a particular test example, each grown tree present in the\n",
    "model votes. The class with majority votes wins, and the test\n",
    "example is assigned to that class.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\"><b>Advantages and Applicability:</b> Decision tree models are\n",
    "intuitive and easy to explain because these are based upon\n",
    "if-else conditions. These algorithms do not require much data\n",
    "preprocessing because they do not make any assumptions\n",
    "about the distribution of data. This fact makes them very useful\n",
    "in identifying the hidden pattern in the dataset. Decision trees\n",
    "do not require normalization and scaling of the dataset. These\n",
    "algorithms require a small amount of training data to estimate\n",
    "the test data.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: justify\"><b>Limitations:</b> Small changes in the dataset can cause the\n",
    "structure of the decision tree to change considerably, causing\n",
    "instability. The training time of decision trees is often higher\n",
    "than relatively simpler models such as logistic regression and\n",
    "Naïve Bayes’.</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
